{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aa46fec",
   "metadata": {},
   "source": [
    "# Measure the 2-point correlation function using DES deep fields data:\n",
    "\n",
    "* Download the DES galaxy catalog ( we choose here one of the SN fields bc there are 3 supernovae fields and 1 COSMOS)\n",
    "* Create a random catalog of galaxies with the same density as the real catalog.\n",
    "* Calculate the number of pairs of galaxies within a given angular separation for both the real and random catalogs.\n",
    "* Divide the number of real pairs by the number of random pairs to obtain the 2-point correlation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccacaf5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb2315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.io.fits as fits\n",
    "import numpy as np\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "from astropy.table import Table,join\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "import treecorr\n",
    "\n",
    "# Get the current user's home directory\n",
    "home_dir = os.path.expanduser('~')\n",
    "\n",
    "# Construct the path to the \"Thesis\" directory on the desktop\n",
    "thesis_path = os.path.join(home_dir, 'Desktop', 'Thesis')\n",
    "\n",
    "\n",
    "# Assuming you have the path to the FITS file stored in thesis_path\n",
    "fits_file_path = os.path.join(thesis_path, \"Y3_deep_fields_DB_wKNN_cat_SN-C3_zm.fits\")  # Replace with your actual file name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a74b33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t= Table.read(fits_file_path)\n",
    "t  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb6d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked = os.path.join(thesis_path, \"SN-C3_masked_cat.fits\")  # Replace with your actual file name\n",
    "\n",
    "t3= Table.read(masked)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e9893",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=join(t,t3,keys='id')\n",
    "\n",
    "\n",
    "t.rename_column('ra_1','ra')\n",
    "t.rename_column('dec_1','dec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c583fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Access the columns directly\n",
    "redshift = t['z']  # Access the 'z' column for redshift\n",
    "declination = t['dec']  # Access the 'dec' column for declination\n",
    "right_ascension = t['ra']  # Access the 'ra' column for right ascension\n",
    "\n",
    "# Optional: explore or analyze the data\n",
    "print(redshift.shape)  # Check the shape of the redshift array\n",
    "print(f\"Minimum redshift: {redshift.min()}\")\n",
    "print(f\"Maximum redshift: {redshift.max()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd09648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select redshift subset \n",
    "subset = (t['z'] > 0.6) & (t['z'] < 0.7) & (t['SM']>10.5)& (t['SM']<11) #solar masses\n",
    "\n",
    "# Extract right ascension (ra) and declination (dec) arrays from the subset\n",
    "ra_subset = t['ra'][subset] #[::100]\n",
    "dec_subset = t['dec'][subset]#[::100]\n",
    "\n",
    "catalog=SkyCoord(ra=ra_subset*u.deg, dec=dec_subset*u.deg)\n",
    "\n",
    "N = len(catalog)\n",
    "\n",
    "\n",
    "\n",
    "#def calculate_separations(cat, N):\n",
    "    #\"\"\"Calculates the angular separation between all pairs of objects in a catalog.\n",
    "\n",
    "    #Args:\n",
    "       # catalog1: An astropy.coordinates.SkyCoord object containing object coordinates.\n",
    "       # N: The number of bins for the separation histogram.\n",
    "\n",
    "   # Returns:\n",
    "      #  A numpy array of size N containing the counts of objects in each separation bin.\n",
    "   # \"\"\"\n",
    "\n",
    "    #separation = cat.separation(cat[:, np.newaxis])\n",
    "\n",
    "    #return separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pairs_in_theta_bin(cat,theta_edges):\n",
    "    \"\"\"Counts the number of pairs of objects in a catalog that have an angular separation\n",
    "    within a specified theta bin.\n",
    "\n",
    "    Args:\n",
    "        catalog: An astropy.coordinates.SkyCoord object containing object coordinates.\n",
    "        theta_bins: A tuple defining the angular separation bin.\n",
    "\n",
    "    Returns:\n",
    "        The number of pairs within the theta bin.\n",
    "    \"\"\"\n",
    "    separation = cat.separation(cat[:, np.newaxis]) #Calculates the angular separation between all pairs of objects in a catalog.\n",
    "    theta_hist,_= np.histogram(np.log10(separation.value),bins=theta_edges)\n",
    "\n",
    "    #for i in range(0,N):\n",
    "        #for j in range(i, N):\n",
    "            #theta = calculate_separations(catalog[i],catalog[j])\n",
    "            #theta_bins = calculate_separations(theta)\n",
    "            \n",
    "            #separation = catalog[i].separation(catalog[j]).to('deg').value\n",
    "            #theta_bins = calculate_separations(SkyCoord(ra=ra_subset * u.deg, dec=dec_subset * u.deg), N=50)\n",
    "\n",
    "            #theta_bins += 1\n",
    "\n",
    "    return theta_hist\n",
    "\n",
    "\n",
    "theta_edges=np.linspace(-2.5,0.25,50) #-2.5 and 0.25 are log of the max and min separation in degrees\n",
    "dd_counts=count_pairs_in_theta_bin(catalog,theta_edges)  \n",
    "print(dd_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4371ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_edges=np.linspace(-2.5,0.25,50)\n",
    "\n",
    "# bin centers\n",
    "theta_cen= (theta_edges[:-1]+theta_edges[1:])/2\n",
    "\n",
    "# Plot the theta bins\n",
    "plt.scatter(theta_cen, dd_counts, label='Theta Bins')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('radius of log deg')\n",
    "plt.ylabel('log of dd_counts')\n",
    "plt.title('')\n",
    "\n",
    "# Show the plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ceb41",
   "metadata": {},
   "source": [
    "## Catalog for randoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe10918",
   "metadata": {},
   "outputs": [],
   "source": [
    "fits_random = os.path.join(thesis_path, \"SN-C3_randoms_ugriz_trim_video.fits\") \n",
    "\n",
    "# Open the FITS file using astropy.io.fits\n",
    "hdulist = fits.open(fits_random)\n",
    "hdulist.info()\n",
    "\n",
    "t2= Table.read(fits_random)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24738cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract right ascension (ra) and declination (dec) arrays from the subset\n",
    "ra_2 = t2['ra'][::1000]\n",
    "dec_2 = t2['dec'][::1000]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d6245",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_catalog=SkyCoord(ra=ra_2*u.deg, dec=dec_2*u.deg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b165c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_2pcf(catalog, random_catalog, theta_edges):\n",
    "    \"\"\"Calculates the two-point correlation function (2PCF).\n",
    "\n",
    "    Args:\n",
    "        catalog: An astropy.coordinates.SkyCoord object containing data points.\n",
    "        random_catalog: An astropy.coordinates.SkyCoord object containing random points.\n",
    "        theta_cen: A numpy array defining the angular separation bin edges.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the 2PCF values for each theta bin.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate DD counts\n",
    "    dd_counts = count_pairs_in_theta_bin(catalog, theta_edges)\n",
    "\n",
    "    # Calculate RR counts\n",
    "    rr_counts = count_pairs_in_theta_bin(random_catalog, theta_edges)\n",
    "    \n",
    "    \n",
    "    #Normalise\n",
    "    norma_dd= dd_counts/np.sum(dd_counts) \n",
    "    norma_rr= rr_counts/np.sum(rr_counts) \n",
    "\n",
    "\n",
    "    # Calculate 2PCF\n",
    "    two_pcf = (norma_dd / norma_rr) - 1 # w_measured\n",
    "\n",
    "    return two_pcf\n",
    "\n",
    "\n",
    "theta_cen= (theta_edges[:-1]+theta_edges[1:])/2\n",
    "\n",
    "two_pcf_result = calculate_2pcf(catalog, random_catalog, theta_edges)\n",
    "\n",
    "print(two_pcf_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25af0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(t['z'],t['SM'],alpha=0.1)\n",
    "plt.xlabel('Redshift z')\n",
    "plt.ylabel('Stellar Mass')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c819d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_theta_cen=10**theta_cen\n",
    "\n",
    "plt.scatter(deg_theta_cen, two_pcf_result)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(r' $ \\theta$ (degrees)')\n",
    "plt.ylabel(r' $w(\\theta)$')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b6e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_theta_cen=10**theta_cen\n",
    "A=2*1e-2  #amplitude, best is 2*1e-2 aka 0.02\n",
    "w_fit= A*deg_theta_cen**(-0.8) #w(theta)\n",
    "\n",
    "plt.scatter(deg_theta_cen, two_pcf_result)\n",
    "plt.plot(deg_theta_cen, w_fit, label='Power Law Fit')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(r' $ \\theta$ (degrees)')\n",
    "plt.ylabel(r' $w(\\theta)$')\n",
    "plt.title(\"Power law fit by hand\")\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbd06a4",
   "metadata": {},
   "source": [
    "#### Note: cut first point for better fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdebf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Define the Peebles & Groth (1975) power law function\n",
    "def power_law(x, r0, gamma):\n",
    "    return (x *r0)**gamma\n",
    "\n",
    "# Fit the power law\n",
    "popt, pcov = curve_fit(power_law, deg_theta_cen[1:], two_pcf_result[1:], p0=[2*1e-2, -0.8])  # Initial guesses \n",
    "\n",
    "# Print fit parameters\n",
    "print(\"Power-law fit parameters:\")\n",
    "print(\"  r0:\", popt[0])\n",
    "print(\"  gamma:\", popt[1])\n",
    "\n",
    "# Create the plot (log scale for y-axis)\n",
    "plt.scatter(deg_theta_cen, two_pcf_result)\n",
    "plt.plot(deg_theta_cen, power_law(deg_theta_cen, *popt), label='Power Law Fit')\n",
    "\n",
    "# Set labels and title\n",
    "# Set labels and title\n",
    "plt.xlabel(r' $ \\theta$ (degrees)')\n",
    "plt.ylabel(r' $w(\\theta)$')\n",
    "plt.title(\"Power law fit using curve_fit\")\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe9f422",
   "metadata": {},
   "source": [
    "## Integral constraint\n",
    "#### Due to the relatively small volume probed by the survey, the integral constraint affects w(θ) at large scales.\n",
    "\n",
    "We adjust the model to take this into account. The correction factor due to the IC can be estimated from the double integration of the true correlation function over the survey area. This integration can be carried out using the random-random pairs from the random catalog following Roche & Eales (1999) where wtrue(θ) is HOD-predicted model. Finally, the model that we fit against the data is simply w(θ) = wtrue(θ) − wIC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b030643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate DD counts\n",
    "dd_counts = count_pairs_in_theta_bin(catalog, theta_edges)\n",
    "\n",
    "# Calculate RR counts\n",
    "rr_counts = count_pairs_in_theta_bin(random_catalog, theta_edges)\n",
    "    \n",
    "#Normalise\n",
    "norma_dd= dd_counts/np.sum(dd_counts) \n",
    "norma_rr= rr_counts/np.sum(rr_counts) \n",
    "\n",
    "# Calculate 2PCF aka w(theta)= DD/RR - 1 \n",
    "two_pcf = (norma_dd / norma_rr) - 1\n",
    "\n",
    "w_IC = np.sum(w_fit * rr_counts / np.sum(rr_counts)) #IC = w_fit * RR(theta) / SUM(RR)\n",
    "\n",
    "#ideally we want to use the w_true in the IC but we can't know it so we use w_fit\n",
    "\n",
    "# w_measured is DD/RR- 1\n",
    "\n",
    "w= two_pcf + np.sum(w_IC) # w_measured + w_IC \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfc14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(deg_theta_cen, w)\n",
    "plt.plot(deg_theta_cen, power_law(deg_theta_cen, *popt), label='Power Law Fit')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(r' $ \\theta$ (degrees)')\n",
    "plt.ylabel(r' $w(\\theta)$')\n",
    "plt.title(\"IC corrected\")\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2de1d2",
   "metadata": {},
   "source": [
    "\n",
    "## Implementing Gaussian Approximation for Errorbars\n",
    "\n",
    "The Gaussian approximation to Poisson errors assumes that, for large counts, the Poisson distribution can be approximated by a normal distribution with mean and variance equal to the Poisson parameter 1  (in this case, the number of pairs). This approximation is reasonable when the number of pairs is sufficiently large.\n",
    "\n",
    "In the Gaussian approximation, the fractional error is 1/sqrt(N pairs). We'll treat the RR part as being noiseless, and all the error as coming from Poisson error on the counting of DD pairs. So the errorbar on the ratio DD/RR is, ( 1 / sqrt(number of  DD pairs) ) * DD/RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0e795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "fractional_error = 1 / np.sqrt(dd_counts)\n",
    "errorbars = fractional_error * (norma_dd / norma_rr)  #So the errorbar on the ratio DD/RR is: ( 1 / sqrt(number of  DD pairs) ) * DD/RR\n",
    "\n",
    "plt.errorbar(deg_theta_cen, w, yerr=errorbars, fmt='o')\n",
    "plt.plot(deg_theta_cen, power_law(deg_theta_cen, *popt), label='Power Law Fit')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "# Print fit parameters\n",
    "print(\"Power-law fit parameters:\")\n",
    "print(\"  r0:\", popt[0])\n",
    "print(\"  gamma:\", popt[1])\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(r' $ \\theta$ (degrees)')\n",
    "plt.ylabel(r' $w(\\theta)$')\n",
    "plt.title(\"IC corrected data\")\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2056853",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(errorbars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87c5ae0",
   "metadata": {},
   "source": [
    "#### See that despite the fact that the number of DD pairs is very high at large separations, the error on DD/RR - 1 is quite large. At small separations, the fact that there are few DD pairs leads to large errors too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a2c666",
   "metadata": {},
   "source": [
    "## Use PyMC and implement IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bbadee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b00cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fit= 2e-2* deg_theta_cen**(-0.8)\n",
    "cuts=(deg_theta_cen>3e-3) & (deg_theta_cen <1) #define valid values\n",
    "X= deg_theta_cen[cuts]\n",
    "Y= w_fit[cuts]\n",
    "Y_err = errorbars[cuts]\n",
    "mask_rr= rr_counts[cuts]\n",
    "\n",
    "\n",
    "# Build the PyMC model\n",
    "with pm.Model() as model:\n",
    "    # put our RR pair counts in tensor form    \n",
    "    rr = pm.ConstantData(\"rr\", mask_rr, dims=\"observation\")\n",
    "\n",
    "    # Define priors\n",
    "    power= pm.Normal(\"power\", mu= -0.8, sigma=10)\n",
    "    A = pm.Normal(\"A\", mu=2e-2, sigma=10)\n",
    "    \n",
    "    Y_pred = A * (X** power)\n",
    "    IC = pm.math.sum(A * (X** power) * rr / pm.math.sum(rr))\n",
    "    \n",
    "    total_sigma=np.sqrt(Y_err**2)\n",
    "    \n",
    "    # Define likelihood\n",
    "    likelihood = pm.Normal(\"Y\", mu=Y_pred-IC, sigma=total_sigma, observed=Y, dims=\"observation\")\n",
    "\n",
    "    # Inference!\n",
    "    # draw 3000 posterior samples using NUTS sampling\n",
    "    idata = pm.sample(3000)\n",
    "\n",
    "#Extract posterior samples\n",
    "power_post_parent= np.array(idata.posterior['power']).flatten()\n",
    "A_post_parent = np.array(idata.posterior['A']).flatten()\n",
    "    \n",
    "power_median = np.median(power_post_parent)\n",
    "A_median = np.median(A_post_parent)\n",
    "\n",
    "print('power_median',power_median)\n",
    "print('A_median', A_median)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee39de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the data and fitted function\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.errorbar(deg_theta_cen, w, yerr=errorbars, fmt='o',label=\"Data + IC \") # w is data + IC\n",
    "plt.plot(deg_theta_cen, power_law(deg_theta_cen, *popt), label='Inital Fit')\n",
    "\n",
    "plt.plot(X, A_median * X**power_median, label=\"Final model\", color=\"red\")\n",
    "plt.xlabel(r' $ \\theta$ (degrees)')\n",
    "plt.ylabel(r' $w(\\theta)$')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.title(\"Power-Law Fit\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808957d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.cosmology import Planck15\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rc('font',**{'family':'serif','size':18})\n",
    "plt.rc('text', usetex=True) # comment out this line if you don't have latex installed\n",
    "\n",
    "\n",
    "z_mean = 0.65\n",
    "\n",
    "\n",
    "def w_th(A, power, theta, rr):\n",
    "    IC = pm.math.sum(A * (theta** power) * rr / pm.math.sum(rr))\n",
    "    print(IC)\n",
    "\n",
    "    return A * (theta**power) - IC #model - IC\n",
    "\n",
    "print(np. array(w_th(A_median, power_median, X, mask_rr)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e75f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_t(A, power, theta, rr):\n",
    "    IC = np.sum(A * (theta** power) * rr / np.sum(rr))\n",
    "    print(IC)\n",
    "\n",
    "    return A * (theta**power) - IC #model - IC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e849abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,7])\n",
    "ax = fig.add_subplot(111)\n",
    "ax.errorbar(X, w[:len(X)], yerr=Y_err, fmt='o', label='SN-C3')\n",
    "ax.plot(X, w_t(A_median, power_median, X, mask_rr),'k:')\n",
    "ax.set_xlabel(r'$\\theta$ (degrees)')\n",
    "ax.set_ylabel(r'$w(\\theta)$')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim([1e-4,10.])\n",
    "ax.set_xlim([8e-4,2.])\n",
    "secax = ax.secondary_xaxis('top',functions=(lambda x: x*Planck15.kpc_proper_per_arcmin(z_mean).value*60./1e3,\n",
    "                                   lambda x: x*Planck15.arcsec_per_kpc_proper(z_mean).value*1e3/3.6e3))\n",
    "secax.set_xlabel(r'Mpc')\n",
    "ax.legend()\n",
    "plt.savefig(\"clustering.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a6db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_err)\n",
    "Y_err.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c59c2",
   "metadata": {},
   "source": [
    "# Improving the estimates of the uncertainties. \n",
    "### Bootstrap Resampling for Covariance Matrix\n",
    "\n",
    "\n",
    "The errors from a Gaussian approximation to Poisson errors are really an underestimate of the errors, and we really want the covariance matrix for our data - because the points are not independent of one another. The treecorr package has some better methods included, but we'll start by performing bootstrap resampling by hand.\n",
    "The idea is that we draw a sample of N objects from our data set, where N is the size of your current sample. But whenever we draw a sample, it is still available to be drawn again a second or third time etc. - you'll see this referred to as drawing randomly with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe7d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "N= len(catalog) #catalog of our subset of galaxies\n",
    "\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dde1141",
   "metadata": {},
   "source": [
    "What np.arange does is construct an array, [0,1,2,3,...N] up to the number given as argument, in this case the size of our data set. Then, np.random.choice selects one the array values at random, and does this as many times as we ask with 'size='. So sample_indices will now contain a set of integers corresponding to index values of our data set.  \n",
    "\n",
    "Our bootstrap sample is then\n",
    "bs_galaxies = data[sample_indices]\n",
    "\n",
    "\n",
    "We then want to recompute the DD and DR pair counts and w(theta) for this bootstrap sample (RR remains the same).\n",
    "Do this 100 times, saving the w(theta) for each bootstrap sample. Then compute the variance per theta bin from those 100 samples.\n",
    "We'll then go on to compute the covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9777a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pairs(catalog, random_catalog, theta_edges):\n",
    "    \n",
    "    separation = catalog.separation(random_catalog[:, np.newaxis]) #Calculates the angular separation between all pairs of objects in a catalog.\n",
    "    pair_counts,_= np.histogram(np.log10(separation.value),bins=theta_edges)\n",
    "\n",
    "\n",
    "    return pair_counts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_samples = []\n",
    "w_theta_bootstrap = []\n",
    "\n",
    "\n",
    "# Loop over bootstrap samples\n",
    "for i in range(100): #100 range\n",
    "    \n",
    "    # Generate random indices for the bootstrap sample\n",
    "    sample_indices = np.random.choice(np.arange(N), size=N) \n",
    "    # Create the bootstrap sample\n",
    "    bs_galaxies = catalog[sample_indices]\n",
    "\n",
    "    # Compute DD, DR, and RR pair counts for the bootstrap sample\n",
    "    dd_counts = count_pairs_in_theta_bin(bs_galaxies, theta_edges)    \n",
    "    #rr_counts = count_pairs_in_theta_bin(random_catalog, theta_edges)\n",
    "    dr_counts = count_pairs(bs_galaxies, random_catalog, theta_edges)\n",
    "\n",
    "    \n",
    "    #Normalise\n",
    "    dd= dd_counts/np.sum(dd_counts) \n",
    "    rr= rr_counts/np.sum(rr_counts) \n",
    "    dr = dr_counts/ np.sum(dr_counts)\n",
    "\n",
    "    # Compute w(theta) for the bootstrap sample\n",
    "    w_theta = (dd -2* dr + rr)/ rr\n",
    "\n",
    "    # Store the bootstrap sample and w(theta) values\n",
    "    #bootstrap_samples.append(bs_galaxies)\n",
    "    w_theta_bootstrap.append(w_theta)\n",
    "\n",
    "# Compute variance per theta bin\n",
    "variance_w_theta = np.var(w_theta_bootstrap, axis=0)\n",
    "#Calculate the covariance matrix\n",
    "covariance = np.cov(w_theta_bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb313de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev= np.std(w_theta_bootstrap, axis=1)\n",
    "print(std_dev)\n",
    "std_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cb3919",
   "metadata": {},
   "outputs": [],
   "source": [
    "Std = np.sqrt(np.diag(covariance))  # Standard errors\n",
    "print(Std)\n",
    "Std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d513bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,7])\n",
    "ax = fig.add_subplot(111)\n",
    "ax.errorbar(X, w[:len(X)], yerr=std_dev[:len(X)], fmt='o', label='SN-C3 with std from bootstrap')\n",
    "ax.plot(X, w_t(A_median, power_median, X, mask_rr),'k:')\n",
    "ax.set_xlabel(r'$\\theta$ (degrees)')\n",
    "ax.set_ylabel(r'$w(\\theta)$')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim([1e-4,10.])\n",
    "ax.set_xlim([8e-4,2.])\n",
    "secax = ax.secondary_xaxis('top',functions=(lambda x: x*Planck15.kpc_proper_per_arcmin(z_mean).value*60./1e3,\n",
    "                                   lambda x: x*Planck15.arcsec_per_kpc_proper(z_mean).value*1e3/3.6e3))\n",
    "secax.set_xlabel(r'Mpc')\n",
    "ax.legend()\n",
    "plt.savefig(\"clustering-by-hand.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32076d49",
   "metadata": {},
   "source": [
    "# Switching from custom pair-counting function to the treecorr package.\n",
    "## NNCorrelation: Count-count correlations\n",
    "\n",
    "Bases: Corr2\n",
    "\n",
    "This class handles the calculation and storage of a 2-point count-count correlation function. i.e. the regular density correlation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b816f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.io.fits as fits\n",
    "import numpy as np\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "from astropy.table import Table,join\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "import treecorr\n",
    "\n",
    "\n",
    "\n",
    "# Get the current user's home directory\n",
    "home_dir = os.path.expanduser('~')\n",
    "\n",
    "# Construct the path to the \"Thesis\" directory on the desktop\n",
    "thesis_path = os.path.join(home_dir, 'Desktop', 'Thesis')\n",
    "\n",
    "\n",
    "# Assuming you have the path to the FITS file stored in thesis_path\n",
    "fits_file_path = os.path.join(thesis_path, \"Y3_deep_fields_DB_wKNN_cat_SN-C3_zm.fits\")  # Replace with your actual file name\n",
    "fits_random = os.path.join(thesis_path, \"SN-C3_randoms_ugriz_trim_video.fits\") \n",
    "masked = os.path.join('/Volumes/data/SN-C3_masked_cat.fits')  \n",
    "\n",
    "\n",
    "t= Table.read(fits_file_path)\n",
    "t2= Table.read(fits_random)\n",
    "t3= Table.read(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fdc882",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=join(t,t3,keys='id')\n",
    "\n",
    "\n",
    "t.rename_column('ra_1','ra')\n",
    "t.rename_column('dec_1','dec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7bca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select redshift subset \n",
    "subset = (t['z'] > 0.6) & (t['z'] < 0.7) & (t['SM']>10.5)& (t['SM']<11) #solar masses\n",
    "\n",
    "# Extract right ascension (ra) and declination (dec) arrays from the subset\n",
    "ra_subset = t['ra'][subset] #[::100]\n",
    "dec_subset = t['dec'][subset]#[::100]\n",
    "\n",
    "\n",
    "ra_2 = t2['ra'][::1000]\n",
    "dec_2 = t2['dec'][::1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eeb8df",
   "metadata": {},
   "source": [
    "## !!! careful i did not choose for the catalog subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb1cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    'ra_col': 'ra',\n",
    "    'dec_col': 'dec',\n",
    "    'ra_units': 'deg',\n",
    "    'dec_units': 'deg',\n",
    "    'min_sep': 0.003,  # Define the minimum separation\n",
    "    'max_sep': 1.78,   # Define the maximum separation\n",
    "    'bin_size': 0.1,# Define the desired bin size (adjust as needed),\n",
    "    'sep_units':'deg'\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#cat1 = treecorr.Catalog(fits_file_path, config, npatch = 50 ) \n",
    "#cat2 = treecorr.Catalog(fits_random, config, npatch = 50 ) \n",
    "ra=ra_subset*u.deg\n",
    "dec=dec_subset*u.deg\n",
    "\n",
    "\n",
    "\n",
    "ra_rand=ra_2*u.deg\n",
    "dec_rand=dec_2*u.deg\n",
    "\n",
    "\n",
    "cat1 = treecorr.Catalog(ra=ra, dec=dec, config=config, npatch=50)  # my catalog subset\n",
    "cat2 = treecorr.Catalog(ra= ra_rand, dec= dec_rand, config=config, npatch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2821b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#treecorr.NNCorrelation class requires two mandatory parameters: min_sep and max_sep.\n",
    "#These parameters define the minimum and maximum separation distances considered when calculating the correlation function.\n",
    "\n",
    "dd = treecorr.NNCorrelation(config) #var_method='bootstrap'\n",
    "rr = treecorr.NNCorrelation(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290a15f8",
   "metadata": {},
   "source": [
    "## kernel always dies here and my notebook also sometimes randomly deletes itself :(("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c0e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.process(cat1)   \n",
    "rr.process(cat2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb458819",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = treecorr.NNCorrelation(config) \n",
    "dr = treecorr.NNCorrelation(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e3938",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.process(cat1, cat2)   \n",
    "dr.process(cat1, cat2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d4485",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dd.write(catalog,rr=rr,dr=dr,rd=rd)# Write out to a file.\n",
    "\n",
    "#dd.calculatexi(dr=dr, rr=rr)\n",
    "\n",
    "dd.calculateXi(rr=rr, dr=dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6352d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_cov = dd.cov  # Can access covariance now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_cov_bs = dd.estimate_cov(method='bootstrap') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd_cov_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ec459",
   "metadata": {},
   "source": [
    "If calculateXi has been called, then the following will also be available:\n",
    "\n",
    "* xi – The correlation function, \n",
    "* varxi – An estimate of the variance of \n",
    "* cov – An estimate of the full covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224cd4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = dd.estimate_cov(method='shot') #  In this case, the returned value will only be the diagonal.\n",
    "err= np.sqrt(diag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13756f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "err.shape\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf618966",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4fb82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w[:len(X)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35826f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,7])\n",
    "ax = fig.add_subplot(111)\n",
    "ax.errorbar(X, w[:len(X)], yerr=err[:len(X)], fmt='o', label='SN-C3 with std using tree corr')\n",
    "ax.plot(X, w_t(A_median, power_median, X, mask_rr),'k:')\n",
    "ax.set_xlabel(r'$\\theta$ (degrees)')\n",
    "ax.set_ylabel(r'$w(\\theta)$')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim([1e-4,10.])\n",
    "ax.set_xlim([8e-4,2.])\n",
    "secax = ax.secondary_xaxis('top',functions=(lambda x: x*Planck15.kpc_proper_per_arcmin(z_mean).value*60./1e3,\n",
    "                                   lambda x: x*Planck15.arcsec_per_kpc_proper(z_mean).value*1e3/3.6e3))\n",
    "secax.set_xlabel(r'Mpc')\n",
    "ax.legend()\n",
    "plt.savefig(\"clustering-with-treecorr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc77b69",
   "metadata": {},
   "source": [
    "# Accounting for the projection in the angular correlation function to infer a 3D physical quantity, r0 (in Mpc).\n",
    "\n",
    "\n",
    "### The angular correlation function (w(theta)) measures the clustering of galaxies on the sky. However, it's a 2D projection of the underlying 3D distribution. To infer the 3D physical quantity r0 (comoving correlation length), we need to deproject w(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c94bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93aac7be",
   "metadata": {},
   "source": [
    "## Convert units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a35603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "\n",
    "cosmo = FlatLambdaCDM(H0=67.8, Om0=0.308)  \n",
    "\n",
    "# Calculate angular diameter distance (d_A)\n",
    "d_A = cosmo.angular_diameter_distance(z=0.65)  # Average redshift \n",
    "\n",
    "\n",
    "# Convert theta from radians to Mpc\n",
    "theta_Mpc = d_A * deg_theta_cen\n",
    "\n",
    "# Convert theta_Mpc to h^-1 Mpc\n",
    "theta_h_inv_Mpc =  theta_Mpc / (cosmo.H0 * u.km / u.s / u.Mpc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8528d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fit_mpc = theta_h_inv_Mpc ** (-0.8)\n",
    "plt.yscale('log')\n",
    "plt.plot(theta_h_inv_Mpc, fit_mpc, label='Power Law Fit')\n",
    "plt.scatter(theta_h_inv_Mpc, two_pcf_result)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(r'$ \\theta(h^{-1} Mpc)$')\n",
    "plt.ylabel( r'$w(\\theta)$')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Power Law \")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a605229d",
   "metadata": {},
   "source": [
    "fit power law and slope of -0.8\n",
    "integral constraint, check in papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86394602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb7245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
